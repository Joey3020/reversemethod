{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "LOAD_DATA = True\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, batch_size, path):\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path\n",
    "        self.current_batch = 0\n",
    "        # load data\n",
    "        input1_pd = pd.read_excel(open(self.path, 'rb'), sheet_name = 'Sheet1', header = None)\n",
    "        input_ten = torch.from_numpy( np.array(input1_pd) ).float()\n",
    "        self.input = input_ten.reshape(-1,1,64,64)\n",
    "        \n",
    "        label = pd.read_excel(open(self.path, 'rb'), sheet_name = 'Sheet2', header = None)\n",
    "        self.label = torch.from_numpy( np.array(label) ).float()\n",
    "    \n",
    "        self.data_size = len(label)\n",
    "        print(self.data_size)\n",
    "    \n",
    "    def getbatchnum(self):\n",
    "        return math.ceil(self.data_size / self.batch_size)\n",
    "    \n",
    "    def getbatch(self):\n",
    "        current_batch = self.current_batch\n",
    "        length = self.batch_size\n",
    "\n",
    "        if current_batch + length <= self.data_size :\n",
    "            data_return = self.input[ current_batch : current_batch + length ]\n",
    "            self.current_batch += length\n",
    "            return data_return, self.label[ current_batch : current_batch + length ]\n",
    "        else:\n",
    "            data_return = self.input[ current_batch : -1 ]\n",
    "            self.current_batch = 0\n",
    "            return data_return, self.label[ current_batch : -1]\n",
    "        \n",
    "    def reset(self): #어디까지 리턴했는지 초기화\n",
    "        self.current_batch = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 24, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #32 x 32 x 24\n",
    "            \n",
    "            nn.Conv2d(24, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #16 x 16 x 32\n",
    "            \n",
    "            nn.Conv2d(32, 40, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(40, 40, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #8 x 8 x 40\n",
    "            \n",
    "            nn.Conv2d(40, 48, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            #4 x 4 x 48\n",
    "            \n",
    "            nn.Conv2d(48, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "            #2 x 2 x 64\n",
    "        )\n",
    "        self.fcc = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 48),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(48, 5)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.cnn(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fcc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "if LOAD_DATA:\n",
    "    trainpath = 'traindata.xlsx'\n",
    "    valpath = 'valdata.xlsx'\n",
    "    testpath = 'testdata.xlsx'\n",
    "    \n",
    "    traindata = DataLoader(batch_size = BATCH_SIZE, path = trainpath)\n",
    "    valdata = DataLoader(batch_size = BATCH_SIZE, path = valpath)\n",
    "    testdata = DataLoader(batch_size = 1, path = testpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "optimizer = optim.Adam(net.parameters(), LEARNING_RATE, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs :  0 loss:  tensor(74.6175, grad_fn=<DivBackward0>)\n",
      "epochs :  1 loss:  tensor(33.0421, grad_fn=<DivBackward0>)\n",
      "epochs :  2 loss:  tensor(29.8571, grad_fn=<DivBackward0>)\n",
      "epochs :  3 loss:  tensor(21.3580, grad_fn=<DivBackward0>)\n",
      "epochs :  4 loss:  tensor(26.2496, grad_fn=<DivBackward0>)\n",
      "epochs :  5 loss:  tensor(21.6995, grad_fn=<DivBackward0>)\n",
      "epochs :  6 loss:  tensor(25.6292, grad_fn=<DivBackward0>)\n",
      "epochs :  7 loss:  tensor(19.5356, grad_fn=<DivBackward0>)\n",
      "epochs :  8 loss:  tensor(17.3498, grad_fn=<DivBackward0>)\n",
      "epochs :  9 loss:  tensor(17.9261, grad_fn=<DivBackward0>)\n",
      "epochs :  10 loss:  tensor(17.8681, grad_fn=<DivBackward0>)\n",
      "epochs :  11 loss:  tensor(17.3151, grad_fn=<DivBackward0>)\n",
      "epochs :  12 loss:  tensor(17.5210, grad_fn=<DivBackward0>)\n",
      "epochs :  13 loss:  tensor(14.4861, grad_fn=<DivBackward0>)\n",
      "epochs :  14 loss:  tensor(16.1159, grad_fn=<DivBackward0>)\n",
      "epochs :  15 loss:  tensor(17.6188, grad_fn=<DivBackward0>)\n",
      "epochs :  16 loss:  tensor(16.2405, grad_fn=<DivBackward0>)\n",
      "epochs :  17 loss:  tensor(16.2230, grad_fn=<DivBackward0>)\n",
      "epochs :  18 loss:  tensor(17.3749, grad_fn=<DivBackward0>)\n",
      "epochs :  19 loss:  tensor(15.3887, grad_fn=<DivBackward0>)\n",
      "epochs :  20 loss:  tensor(17.4942, grad_fn=<DivBackward0>)\n",
      "epochs :  21 loss:  tensor(14.8342, grad_fn=<DivBackward0>)\n",
      "epochs :  22 loss:  tensor(15.6011, grad_fn=<DivBackward0>)\n",
      "epochs :  23 loss:  tensor(16.5444, grad_fn=<DivBackward0>)\n",
      "epochs :  24 loss:  tensor(15.3190, grad_fn=<DivBackward0>)\n",
      "epochs :  25 loss:  tensor(16.2513, grad_fn=<DivBackward0>)\n",
      "epochs :  26 loss:  tensor(14.0132, grad_fn=<DivBackward0>)\n",
      "epochs :  27 loss:  tensor(14.8484, grad_fn=<DivBackward0>)\n",
      "epochs :  28 loss:  tensor(13.4291, grad_fn=<DivBackward0>)\n",
      "epochs :  29 loss:  tensor(14.6110, grad_fn=<DivBackward0>)\n",
      "epochs :  30 loss:  tensor(12.9884, grad_fn=<DivBackward0>)\n",
      "epochs :  31 loss:  tensor(14.5928, grad_fn=<DivBackward0>)\n",
      "epochs :  32 loss:  tensor(14.0542, grad_fn=<DivBackward0>)\n",
      "epochs :  33 loss:  tensor(13.3265, grad_fn=<DivBackward0>)\n",
      "epochs :  34 loss:  tensor(13.8422, grad_fn=<DivBackward0>)\n",
      "epochs :  35 loss:  tensor(15.4859, grad_fn=<DivBackward0>)\n",
      "epochs :  36 loss:  tensor(12.9915, grad_fn=<DivBackward0>)\n",
      "epochs :  37 loss:  tensor(12.6065, grad_fn=<DivBackward0>)\n",
      "epochs :  38 loss:  tensor(15.9744, grad_fn=<DivBackward0>)\n",
      "epochs :  39 loss:  tensor(12.2174, grad_fn=<DivBackward0>)\n",
      "epochs :  40 loss:  tensor(11.8085, grad_fn=<DivBackward0>)\n",
      "epochs :  41 loss:  tensor(14.2024, grad_fn=<DivBackward0>)\n",
      "epochs :  42 loss:  tensor(15.3567, grad_fn=<DivBackward0>)\n",
      "epochs :  43 loss:  tensor(13.5566, grad_fn=<DivBackward0>)\n",
      "epochs :  44 loss:  tensor(16.8203, grad_fn=<DivBackward0>)\n",
      "epochs :  45 loss:  tensor(14.9932, grad_fn=<DivBackward0>)\n",
      "epochs :  46 loss:  tensor(14.7146, grad_fn=<DivBackward0>)\n",
      "epochs :  47 loss:  tensor(13.0111, grad_fn=<DivBackward0>)\n",
      "epochs :  48 loss:  tensor(15.7646, grad_fn=<DivBackward0>)\n",
      "epochs :  49 loss:  tensor(11.7345, grad_fn=<DivBackward0>)\n",
      "epochs :  50 loss:  tensor(12.4383, grad_fn=<DivBackward0>)\n",
      "epochs :  51 loss:  tensor(14.1271, grad_fn=<DivBackward0>)\n",
      "epochs :  52 loss:  tensor(13.8844, grad_fn=<DivBackward0>)\n",
      "epochs :  53 loss:  tensor(12.9810, grad_fn=<DivBackward0>)\n",
      "epochs :  54 loss:  tensor(11.3283, grad_fn=<DivBackward0>)\n",
      "epochs :  55 loss:  tensor(11.9334, grad_fn=<DivBackward0>)\n",
      "epochs :  56 loss:  tensor(10.8452, grad_fn=<DivBackward0>)\n",
      "epochs :  57 loss:  tensor(12.7266, grad_fn=<DivBackward0>)\n",
      "epochs :  58 loss:  tensor(16.9209, grad_fn=<DivBackward0>)\n",
      "epochs :  59 loss:  tensor(13.3045, grad_fn=<DivBackward0>)\n",
      "epochs :  60 loss:  tensor(15.6020, grad_fn=<DivBackward0>)\n",
      "epochs :  61 loss:  tensor(9.9828, grad_fn=<DivBackward0>)\n",
      "epochs :  62 loss:  tensor(12.6400, grad_fn=<DivBackward0>)\n",
      "epochs :  63 loss:  tensor(10.9620, grad_fn=<DivBackward0>)\n",
      "epochs :  64 loss:  tensor(12.0132, grad_fn=<DivBackward0>)\n",
      "epochs :  65 loss:  tensor(11.0048, grad_fn=<DivBackward0>)\n",
      "epochs :  66 loss:  tensor(11.1447, grad_fn=<DivBackward0>)\n",
      "epochs :  67 loss:  tensor(9.9876, grad_fn=<DivBackward0>)\n",
      "epochs :  68 loss:  tensor(9.4431, grad_fn=<DivBackward0>)\n",
      "epochs :  69 loss:  tensor(8.8170, grad_fn=<DivBackward0>)\n",
      "epochs :  70 loss:  tensor(10.4481, grad_fn=<DivBackward0>)\n",
      "epochs :  71 loss:  tensor(9.2621, grad_fn=<DivBackward0>)\n",
      "epochs :  72 loss:  tensor(8.7508, grad_fn=<DivBackward0>)\n",
      "epochs :  73 loss:  tensor(10.7020, grad_fn=<DivBackward0>)\n",
      "epochs :  74 loss:  tensor(10.0657, grad_fn=<DivBackward0>)\n",
      "epochs :  75 loss:  tensor(9.2319, grad_fn=<DivBackward0>)\n",
      "epochs :  76 loss:  tensor(11.9143, grad_fn=<DivBackward0>)\n",
      "epochs :  77 loss:  tensor(15.4280, grad_fn=<DivBackward0>)\n",
      "epochs :  78 loss:  tensor(11.7183, grad_fn=<DivBackward0>)\n",
      "epochs :  79 loss:  tensor(12.2404, grad_fn=<DivBackward0>)\n",
      "epochs :  80 loss:  tensor(12.3154, grad_fn=<DivBackward0>)\n",
      "epochs :  81 loss:  tensor(9.5264, grad_fn=<DivBackward0>)\n",
      "epochs :  82 loss:  tensor(7.8227, grad_fn=<DivBackward0>)\n",
      "epochs :  83 loss:  tensor(8.5165, grad_fn=<DivBackward0>)\n",
      "epochs :  84 loss:  tensor(8.5992, grad_fn=<DivBackward0>)\n",
      "epochs :  85 loss:  tensor(8.3561, grad_fn=<DivBackward0>)\n",
      "epochs :  86 loss:  tensor(8.4521, grad_fn=<DivBackward0>)\n",
      "epochs :  87 loss:  tensor(8.3839, grad_fn=<DivBackward0>)\n",
      "epochs :  88 loss:  tensor(10.9536, grad_fn=<DivBackward0>)\n",
      "epochs :  89 loss:  tensor(7.9139, grad_fn=<DivBackward0>)\n",
      "epochs :  90 loss:  tensor(10.4368, grad_fn=<DivBackward0>)\n",
      "epochs :  91 loss:  tensor(8.3040, grad_fn=<DivBackward0>)\n",
      "epochs :  92 loss:  tensor(9.2163, grad_fn=<DivBackward0>)\n",
      "epochs :  93 loss:  tensor(9.8598, grad_fn=<DivBackward0>)\n",
      "epochs :  94 loss:  tensor(9.8538, grad_fn=<DivBackward0>)\n",
      "epochs :  95 loss:  tensor(11.1030, grad_fn=<DivBackward0>)\n",
      "epochs :  96 loss:  tensor(10.1525, grad_fn=<DivBackward0>)\n",
      "epochs :  97 loss:  tensor(7.9523, grad_fn=<DivBackward0>)\n",
      "epochs :  98 loss:  tensor(9.2241, grad_fn=<DivBackward0>)\n",
      "epochs :  99 loss:  tensor(8.7444, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost_list = []\n",
    "for epoch in range(EPOCHS):\n",
    "        cumcost = 0\n",
    "        total = 0\n",
    "        for idx in range(traindata.getbatchnum()):\n",
    "            x, y = traindata.getbatch()\n",
    "            x = x.view(-1, 1, 64, 64)\n",
    "            \n",
    "            net.zero_grad()\n",
    "            pred = net(x)\n",
    "            loss = loss_function(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cumcost += loss\n",
    "            total += 1\n",
    "        cost_list.append(cumcost / total)\n",
    "        print(\"epochs : \", epoch, \"loss: \", cumcost / total)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13.9004,  8.3020,  3.1929,  0.9460,  5.3529]])\n",
      "tensor([[-0.3174, -0.8380, -1.6982, -0.2449, -1.0533]])\n",
      "tensor([[ 7.2843, 11.5200,  0.4819,  3.1730,  4.9455]])\n",
      "tensor([[ -0.8388,  -0.0437, -12.5304,   0.2424,  -0.0867]])\n",
      "tensor([[13.8007,  9.4433, 10.4321,  5.6971, 11.9657]])\n",
      "tensor([[-0.0584, -0.3589,  0.3969, -0.3301,  0.0176]])\n",
      "tensor([[ 6.1393, 14.7856,  5.2098,  0.7488,  7.5268]])\n",
      "tensor([[-1.0322,  0.1609, -0.1838, -1.9622, -0.3712]])\n",
      "tensor([[ 8.2565, 11.3021,  1.9015,  6.5539,  9.4442]])\n",
      "tensor([[-0.5499, -0.2553, -1.6463,  0.7185, -0.3646]])\n",
      "tensor([[10.9988,  9.4843,  0.3896,  4.8732,  6.7082]])\n",
      "tensor([[ -0.2882,  -0.3568, -13.4208,   0.2287,  -0.7635]])\n",
      "tensor([[ 6.0805,  9.5988,  2.7416,  5.2903, 12.2757]])\n",
      "tensor([[-0.4125,  0.0444, -0.7418,  0.3853, -0.0841]])\n",
      "tensor([[12.0085, 13.7224,  0.6267,  3.0145,  7.4350]])\n",
      "tensor([[ 0.0413,  0.1830, -8.8398,  1.1398,  0.0216]])\n",
      "tensor([[14.5853, 12.9005,  6.5907,  4.3014,  1.8273]])\n",
      "tensor([[-0.1664, -0.0583,  0.2636, -0.2917, -2.7049]])\n",
      "tensor([[12.4091, 10.0679,  2.4809,  4.3010,  3.3617]])\n",
      "tensor([[-0.0923, -0.3814, -1.5462, -0.3033, -0.5841]])\n"
     ]
    }
   ],
   "source": [
    "a_error = []\n",
    "b_error = []\n",
    "x_error = []\n",
    "y_error = []\n",
    "d_error = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(testdata.getbatchnum()):\n",
    "        data, label = testdata.getbatch()\n",
    "        data = data.view(-1, 1, 64, 64)\n",
    "        pred = net(data)\n",
    "        print(label)\n",
    "        \n",
    "        error = (label - pred) / label\n",
    "        print(error)\n",
    "        predictions.append(pred)\n",
    "        a_error.append(error[0][0])\n",
    "        b_error.append(error[0][1])\n",
    "        x_error.append(error[0][2])\n",
    "        y_error.append(error[0][3])\n",
    "        d_error.append(error[0][4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
